{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Copy of deep-learning-final-project.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/asmundur/deeplearning-finalproject/blob/master/Copy_of_deep_learning_final_project.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "metadata": {
        "id": "-ZwGhCU_HpsU",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#code based on kernel https://www.kaggle.com/hrmello/flow-from-dataframe-a-memory-friendly-approach\n",
        "\n",
        "\n",
        "from googleapiclient.discovery import build\n",
        "import io, os, cv2, csv\n",
        "import numpy as np\n",
        "from sklearn.preprocessing import LabelBinarizer\n",
        "from googleapiclient.http import MediaIoBaseDownload\n",
        "from google.colab import auth\n",
        "\n",
        "import pandas as pd \n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.image as mplimg\n",
        "from matplotlib.pyplot import imshow\n",
        "from skimage.io import imread\n",
        "\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn.preprocessing import OneHotEncoder\n",
        "\n",
        "from keras import layers\n",
        "from keras.preprocessing import image\n",
        "from keras.applications.imagenet_utils import preprocess_input\n",
        "from keras.layers import Input, Dense, Activation, BatchNormalization, Flatten, Conv2D\n",
        "from keras.layers import AveragePooling2D, MaxPooling2D, Dropout, GlobalMaxPooling2D, GlobalAveragePooling2D, Concatenate\n",
        "from keras.models import Model\n",
        "from keras.applications.vgg16 import VGG16\n",
        "from keras.callbacks import EarlyStopping, ModelCheckpoint\n",
        "\n",
        "import keras.backend as K\n",
        "from keras.models import Sequential\n",
        "\n",
        "import warnings\n",
        "warnings.simplefilter(\"ignore\", category=DeprecationWarning)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "DKOz-CWJTn5M",
        "colab_type": "code",
        "outputId": "9f7813af-2aad-4b9f-c1ef-ac7551781706",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 230
        }
      },
      "cell_type": "code",
      "source": [
        "auth.authenticate_user()\n",
        "drive_service = build('drive', 'v3')\n",
        "results = drive_service.files().list(\n",
        "        q=\"name = 'kaggle.json'\", fields=\"files(id)\").execute()\n",
        "kaggle_api_key = results.get('files', [])\n",
        "filename = \"/content/.kaggle/kaggle.json\"\n",
        "os.makedirs(os.path.dirname(filename), exist_ok=True)\n",
        "request = drive_service.files().get_media(fileId=kaggle_api_key[0]['id'])\n",
        "fh = io.FileIO(filename, 'wb')\n",
        "downloader = MediaIoBaseDownload(fh, request)\n",
        "done = False\n",
        "while done is False:\n",
        "    status, done = downloader.next_chunk()\n",
        "    print(\"Download %d%%.\" % int(status.progress() * 100))\n",
        "os.chmod(filename, 600)\n",
        "\n",
        "\n",
        "\n",
        "!pip install kaggle\n"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Download 100%.\n",
            "Requirement already satisfied: kaggle in /usr/local/lib/python3.6/dist-packages (1.5.1.1)\n",
            "Requirement already satisfied: urllib3<1.23.0,>=1.15 in /usr/local/lib/python3.6/dist-packages (from kaggle) (1.22)\n",
            "Requirement already satisfied: six>=1.10 in /usr/local/lib/python3.6/dist-packages (from kaggle) (1.11.0)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.6/dist-packages (from kaggle) (2018.11.29)\n",
            "Requirement already satisfied: python-dateutil in /usr/local/lib/python3.6/dist-packages (from kaggle) (2.5.3)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.6/dist-packages (from kaggle) (2.18.4)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.6/dist-packages (from kaggle) (4.28.1)\n",
            "Requirement already satisfied: python-slugify in /usr/local/lib/python3.6/dist-packages (from kaggle) (1.2.6)\n",
            "Requirement already satisfied: chardet<3.1.0,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests->kaggle) (3.0.4)\n",
            "Requirement already satisfied: idna<2.7,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests->kaggle) (2.6)\n",
            "Requirement already satisfied: Unidecode>=0.04.16 in /usr/local/lib/python3.6/dist-packages (from python-slugify->kaggle) (1.0.23)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "VeVidkRl2UGi",
        "colab_type": "code",
        "outputId": "d515e8c2-c279-48f7-81b0-3b713d16361b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 226
        }
      },
      "cell_type": "code",
      "source": [
        "# if not os.path.exists('./data'):\n",
        "!cp /content/.kaggle/kaggle.json /root/.kaggle/kaggle.json\n",
        "if (not os.path.exists('./train')) and (not os.path.exists('./test')):\n",
        "  !kaggle competitions download -c humpback-whale-identification\n",
        "  !unzip -q train.zip -d train\n",
        "  !unzip -q test.zip -d test\n",
        "  !ls train | wc -l\n",
        "  !ls test | wc -l\n",
        "  \n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading sample_submission.csv to /content\n",
            "\r  0% 0.00/498k [00:00<?, ?B/s]\n",
            "100% 498k/498k [00:00<00:00, 33.3MB/s]\n",
            "Downloading train.csv to /content\n",
            "  0% 0.00/594k [00:00<?, ?B/s]\n",
            "100% 594k/594k [00:00<00:00, 38.9MB/s]\n",
            "Downloading test.zip to /content\n",
            " 99% 1.34G/1.35G [00:21<00:00, 73.0MB/s]\n",
            "100% 1.35G/1.35G [00:21<00:00, 67.3MB/s]\n",
            "Downloading train.zip to /content\n",
            " 15% 640M/4.16G [00:13<01:01, 61.5MB/s]"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "VZOIpdzcQiRU",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# #!ls -a /root/.kaggle\n",
        "\n",
        "# #!cp /content/.kaggle/kaggle.json /root/.kaggle/kaggle.json\n",
        "# if not os.path.exists('./train'):\n",
        "#   !unzip -q train.zip -d train\n",
        "\n",
        "# !ls train | wc -l\n",
        "\n",
        "# if not os.path.exists('./test'):\n",
        "#   !unzip -q test.zip -d test\n",
        "\n",
        "# !ls test | wc -l"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "UNnACYh8KRgj",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Data preparation"
      ]
    },
    {
      "metadata": {
        "id": "B8iSkEPioXWJ",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "train_df = pd.read_csv(\"./train.csv\")\n",
        "img_path = './train/'\n",
        "\n",
        "#get the first 5 whale images\n",
        "images = [(whale_img, whale_label) for (whale_img, whale_label) in zip(train_df.Image[:5], train_df.Id[:5])]\n",
        "\n",
        "fig, m_axs = plt.subplots(1, len(images), figsize = (20, 10))\n",
        "#show the images and label them\n",
        "for ii, c_ax in enumerate(m_axs):\n",
        "    c_ax.imshow(imread(os.path.join(img_path,images[ii][0])))\n",
        "    c_ax.set_title(images[ii][1])\n",
        "\n",
        "#how many photos of each whail    \n",
        "#train_df.Id.value_counts()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "s9hXK_9_JPZ2",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Make test directory\n",
        "\n",
        "from distutils.dir_util import copy_tree\n",
        "\n",
        "#create directories\n",
        "if not os.path.exists('./test_folder'):\n",
        "  os.mkdir('./test_folder')\n",
        "  os.mkdir('./test_folder/test_images')\n",
        "\n",
        "# copy subdirectory example\n",
        "fromDirectory = img_path\n",
        "toDirectory = \"test_folder/test_images\"\n",
        "\n",
        "copy_tree(fromDirectory, toDirectory, verbose=0)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "YTp9YIPxXHb-",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#Image size\n",
        "sz = 100\n",
        "\n",
        "# create generators\n",
        "datagen=image.ImageDataGenerator(rescale=1./255, validation_split = 0.1)\n",
        "test_datagen = image.ImageDataGenerator(rescale=1./255)\n",
        "\n",
        "train_generator=datagen.flow_from_dataframe(\n",
        "    dataframe=train_df, directory=img_path, \n",
        "    x_col=\"Image\", y_col=\"Id\", has_ext=True, seed = 42,\n",
        "    class_mode=\"categorical\", target_size=(sz,sz), batch_size=32, subset = \"training\")\n",
        "\n",
        "validation_generator = datagen.flow_from_dataframe(dataframe=train_df, directory=img_path, \n",
        "    x_col=\"Image\", y_col=\"Id\", has_ext=True, seed = 42,\n",
        "    class_mode=\"categorical\", target_size=(sz,sz), batch_size=32, subset = \"validation\")\n",
        "\n",
        "test_generator = test_datagen.flow_from_directory(directory=\"test_folder\", \n",
        "    seed = 42, class_mode=None, target_size=(sz,sz), batch_size=1, shuffle = False)\n",
        "\n",
        "STEP_SIZE_TRAIN=train_generator.n//train_generator.batch_size\n",
        "STEP_SIZE_VALID=validation_generator.n//validation_generator.batch_size"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "34SVxT6qJH8V",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#Training metrices\n",
        "from keras.metrics import top_k_categorical_accuracy\n",
        "\n",
        "''' the function top_5_accuracy is from Peter's kernel:\n",
        "    https://www.kaggle.com/pestipeti/keras-cnn-starter\n",
        "'''\n",
        "def top_5_accuracy(y_true, y_pred):\n",
        "    return top_k_categorical_accuracy(y_true, y_pred, k=5)\n",
        "\n",
        "  \n",
        "chanDim = -1 # change this to 1 if you're using theano as a backend\n",
        "#Create the model\n",
        "  \n",
        "model = Sequential()\n",
        "\n",
        "layer = Conv2D(filters = 32, kernel_size = 4, padding = 'same', activation = 'relu', input_shape = (100, 100, 3))\n",
        "model.add(layer)\n",
        "model.add(Dropout(0.5))\n",
        "# print(model.output_shape)\n",
        "# model.add(GlobalMaxPooling2D('channels_first')) \n",
        "# print(model.output_shape)\n",
        "# model.add(Flatten())\n",
        "# Layer 1\n",
        "# model.add(Conv2D(32, (3, 3), padding=\"same\"))\n",
        "# model.add(Activation(\"relu\"))\n",
        "# model.add(BatchNormalization(axis=chanDim))\n",
        "\n",
        "# print(model.output_shape)\n",
        "# # Layer 2\n",
        "model.add(Dropout(0.25))\n",
        "model.add(AveragePooling2D(pool_size=(3, 3), input_shape=model.output_shape))\n",
        "\n",
        "\n",
        "# Layer 3\n",
        "model.add(Conv2D(64, (3, 3), padding=\"same\"))\n",
        "model.add(Activation(\"relu\"))\n",
        "model.add(BatchNormalization(axis=chanDim))\n",
        "model.add(Dropout(0.25))\n",
        "model.add(AveragePooling2D(pool_size=(3, 3), input_shape=model.output_shape))\n",
        "\n",
        "\n",
        "# Layer 4\n",
        "model.add(Conv2D(128, (3, 3), padding=\"same\"))\n",
        "model.add(Activation(\"relu\"))\n",
        "model.add(BatchNormalization(axis=chanDim))\n",
        "model.add(Dropout(0.25))\n",
        "model.add(AveragePooling2D(pool_size=(3, 3), input_shape=model.output_shape))\n",
        "\n",
        "# Layer 5\n",
        "model.add(Conv2D(256, (3, 3), padding=\"same\"))\n",
        "model.add(Activation(\"relu\"))\n",
        "model.add(BatchNormalization(axis=chanDim))\n",
        "model.add(Dropout(0.25))\n",
        "model.add(AveragePooling2D(pool_size=(3, 3), input_shape=model.output_shape))\n",
        "\n",
        "# Layer 6\n",
        "# model.add(Conv2D(512, (3, 3), padding=\"same\"))\n",
        "# model.add(Activation(\"relu\"))\n",
        "# model.add(BatchNormalization(axis=chanDim))\n",
        "# model.add(Dropout(0.25))\n",
        "# model.add(AveragePooling2D(pool_size=(3, 3), input_shape=model.output_shape))\n",
        "\n",
        "# # Layer 7\n",
        "# model.add(Conv2D(1024, (3, 3), padding=\"same\"))\n",
        "# model.add(Activation(\"relu\"))\n",
        "# model.add(BatchNormalization(axis=chanDim))\n",
        "# model.add(Dropout(0.25))\n",
        "# model.add(AveragePooling2D(pool_size=(3, 3), input_shape=model.output_shape))\n",
        "\n",
        "# # Layer 7\n",
        "# model.add(Conv2D(2048, (3, 3), padding=\"same\"))\n",
        "# model.add(Activation(\"relu\"))\n",
        "# model.add(BatchNormalization(axis=chanDim))\n",
        "# model.add(Dropout(0.25))\n",
        "# model.add(AveragePooling2D(pool_size=(3, 3), input_shape=model.output_shape))\n",
        "\n",
        "\n",
        "model.add(GlobalMaxPooling2D('channels_first'))\n",
        "\n",
        "model.add(Dense(5005, activation = 'softmax'))\n",
        "# model.summary()\n",
        "\n",
        "model.compile(optimizer= 'adam', loss='categorical_crossentropy', metrics=['accuracy', top_5_accuracy])\n",
        "\n",
        "# early_stopping = EarlyStopping(monitor = 'val_loss', patience = 5)\n",
        "checkpointer = ModelCheckpoint(filepath='weights.hdf5', \n",
        "                               verbose=1, save_best_only=True)\n",
        "\n",
        "early_stopping = EarlyStopping(monitor = 'val_loss', patience = 5)\n",
        "\n",
        "\n",
        "# Train the model\n",
        "model.fit_generator(generator=train_generator,\n",
        "                    steps_per_epoch=STEP_SIZE_TRAIN,\n",
        "                    validation_data=validation_generator,\n",
        "                    validation_steps=STEP_SIZE_VALID,\n",
        "                    epochs=30, callbacks = [checkpointer, early_stopping])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "pVhSMNOiISB4",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#we need to use .reset() here otherwise\n",
        "#the other of predictions will be different\n",
        "#then the expected\n",
        "test_generator.reset()\n",
        "pred = model.predict_generator(test_generator,verbose = 1,steps=7960)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "1j-H-2-dbDmT",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "'''This filters only the top 5 possible ids of an image'''\n",
        "pred_sorted = np.argsort(-pred, axis = 1)[:,:5]\n",
        "pred_sorted"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "u6UpFh34bHf9",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "'''\n",
        "Now we generate a map of each \n",
        "index to an Id on the format \n",
        "{\n",
        "0: 'w_f48451c',\n",
        "1: 'w_c3d896a',\n",
        "2: 'w_20df2c5',\n",
        "...\n",
        "}\n",
        "'''\n",
        "labels = (train_generator.class_indices)\n",
        "labels = dict((v,k) for k,v in labels.items())"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "Ybd0m3NmbMcR",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "'''\n",
        "Here we prepare pred_ids, which is a list of lists containing \n",
        "the top 5 ids by name. For example, w_13ae3d4. \n",
        "'''\n",
        "from tqdm import tqdm\n",
        "#create empty list\n",
        "pred_ids = list()\n",
        "for i,row in enumerate(tqdm(pred_sorted)):\n",
        "    #create a temporary list to store the ids for a given image\n",
        "    temp_list = []\n",
        "    for j,value in enumerate(row):\n",
        "        #for each index in pred_sorted, append the real Id in temp_list\n",
        "        temp_list.append(labels[row[j]])\n",
        "    #append all 5 ids for a given image to pred_ids\n",
        "    #effectively creating a similar list to pred_sorted\n",
        "    #but with the real ids\n",
        "    pred_ids.append(temp_list)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "0RSHA-2obP9c",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "'''create the final predictions by using all ids in a single string'''\n",
        "final_preds = []\n",
        "for i,top_5_ids in enumerate(pred_ids):\n",
        "    final_preds.append(' '.join(pred_ids[i]))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "k5pczIHsbSfd",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "'''delete the files on disk - otherwise the Kaggle kernel will throw an error'''\n",
        "import shutil\n",
        "shutil.rmtree('test_folder', ignore_errors=True)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "Gn6YL3SZbXI5",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# my_df_new = my_df[[\"c\",\"a\",\"b\"]]\n",
        "\n",
        "submission = pd.DataFrame({\"Image\": os.listdir('./test'), \"Id\": final_preds})\n",
        "submission.to_csv(\"submission.csv\", index = False, columns=[\"Image\",\"Id\"])\n",
        "submission.head()\n",
        "\n",
        "!kaggle competitions submit -c humpback-whale-identification -f submission.csv -m \"Message\""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "yTJoUsNDjDP0",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}